**Timestamp:** Tue Nov 11, 2025 • 3:34 PM ET

You’re not crazy—RAG that “reverts” is almost always a **state + control problem**, not a model IQ problem. The fix is to treat question-generation like a **deterministic state machine** that’s merely *informed* by retrieval, not driven by it.

Below is a tight, field-tested blueprint to stop the spinning and get stable, high-quality scope-creation questions.

---

# 1) Separate concerns: Planner vs. Retriever vs. Realizer

* **Planner (finite-state controller):** Decides *what category of question comes next* (e.g., “Project Type → Site → Size → Structure → Materials → Code/Permits → Access → Schedule/Budget”). It tracks required fields and completion.
* **Retriever (RAG):** Supplies domain facts/examples *only when the planner asks*, e.g., “show examples of composite deck joist spacing” or “local frost depth table.”
* **Realizer (LLM surface):** Turns the planner’s “question intent” into a user-friendly, one-line question + options.

> If you let retrieval influence *planning*, it will meander. Retrieval should only support the *content* of the current step, never the *order*.

---

# 2) Make a tiny **Question DSL** (schema) the model must obey

This kills regression because you always persist/restore a compact state.

```json
{
  "project_type": "deck | landscaping | hvac | roofing",
  "required_fields": [
    {"key": "deck.project_kind", "desc": "new | replace | repair"},
    {"key": "deck.location", "desc": "attached | detached | pool | slope"},
    {"key": "deck.size", "desc": "sqft or S/M/L"},
    {"key": "deck.elevation", "desc": "<=30in | 30-60in | >60in"},
    {"key": "deck.material", "desc": "PT | cedar | composite | PVC"},
    {"key": "deck.stairs", "desc": "yes/no"},
    {"key": "deck.railings", "desc": "yes/no"},
    {"key": "deck.permit_required", "desc": "yes/no/unknown"},
    {"key": "deck.access", "desc": "easy | narrow gate | through house"}
  ],
  "answers": { "deck.project_kind": null, "...": null },
  "next_question_intent": "which field to collect next",
  "history": [
    {"key":"deck.project_kind","q":"Is this new, replace, or repair?","a":"new"}
  ]
}
```

Your agent *always*:

1. Looks at `required_fields` minus `answers`.
2. Picks the next missing key using a **priority map**.
3. Asks exactly one question for that key.
4. Updates state.
5. Repeats.

No wandering, ever.

---

# 3) Planning policy (deterministic)

Create a fixed **priority order** per service. Example for deck:

```
Order: project_kind → location → size → elevation → material → stairs → railings → foundation/soil → utilities → permit → access → timeline → budget
Backfill rules: if material=composite then joist_spacing=12" OC
Stop condition: all required fields answered or user says “quote”
```

Keep this policy in code (YAML/JSON), **not** in the prompt.

---

# 4) Retrieval only on “evidence-needed” steps

Examples:

* If user chooses “composite,” the **retriever** fetches manufacturer joist-spacing charts and regional frost depth; the **realizer** then asks:

  * “Composite usually needs closer joist spacing (often 12” OC). Shall we set joists to 12” OC and beam spacing ≤ 6–8 ft?”
* If elevation > 30", retriever pulls code snippets for rail height; realizer asks:

  * “Local code typically requires ~36–42” guards above 30”. Would you like me to add railings to the scope?”

> Retrieval feeds content, but the *planner* still controls the next field.

---

# 5) Chunking & metadata (to keep RAG precise)

* **Chunk by concept**, not by page length: “Joist spacing,” “Guardrail height,” “Footing depth,” “Material pros/cons,” “Permit triggers,” “Access constraints,” etc.
* Add **metadata**: `service=deck`, `topic=joist_spacing`, `region=VA`, `code_year=2021`, `source_type=code|mfr|guide`.
* Use **hybrid retrieval** (BM25 + embeddings) and a **re-ranker**. Filter by `service` and `region` before scoring.

---

# 6) Prompt scaffolding (three small, fixed prompts)

### a) Planner (system)

> “You are a deterministic planner. Given state JSON, return the next **missing** field key from the priority list and a short rationale. Never ask content; only return `{next_key: "...", rationale: "..."}`.”

### b) Retriever (tool)

Given `{next_key, project_type, region}`, fetch 3–5 high-signal passages (with IDs). No LLM here—just retrieval + rerank.

### c) Realizer (system)

> “You produce one user-facing question. Input: `next_key`, minimal context (user answers), and retrieved snippets. Output a single line question + up to 5 options. Max 180 chars. Add a brief 1-sentence guidance if helpful.”

This keeps messages compact and prevents context blowouts that cause “revert.”

---

# 7) Guardrails that stop drifting

* **Temperature**: 0.0–0.2 for planner/realizer.
* **Max turns** per session: e.g., 18 questions, then summarize and propose a draft scope.
* **Strict JSON I/O** for planner state; validate on every turn.
* **State persistence**: write `answers`, `history`, and `next_key` to a DB after every question. On resume, reload state first, then call planner.
* **Banned actions**: the realizer cannot change the order or add new fields; only the planner can request fields.

---

# 8) Anti-regression: why it “reverts” and how to stop it

Common causes & fixes:

* **Lost state** (page refresh, token limits) → Persist state server-side every turn; reload before inference.
* **Context overflow** → Keep prompts tiny; never pass entire retrieval corpus; only 2–3 snippets.
* **Prompt drift** → Version every prompt (e.g., `planner_v3.2`); store alongside sessions; hash to detect unintended changes.
* **Non-determinism** → Fix temperature, limit top-p, remove “creative” wording in planner.
* **Retriever noise** → Require metadata filters first (`service=deck`, `region=VA`), then rerank.

---

# 9) Gold set + eval harness (fast)

* Build ~30 **scripted scenarios** (e.g., “Deck 20×20, composite, attached, 36” height, narrow gate”).
* For each scenario, assert:

  * The **exact sequence of keys** asked (order fidelity).
  * **Number of questions** (<= target).
  * **Coverage** (100% of required_fields filled).
  * **Advice checks** (if elevation>30", railings asked).
* Run nightly. If any prompt/embedding change breaks order or coverage, fail CI.

---

# 10) Minimal implementation plan (one week)

**Day 1–2**

* Define question DSL + per-service priority YAML.
* Implement planner microservice returning `next_key`.

**Day 3**

* Implement retriever with strict filters + rerank; return 2–3 snippets.

**Day 4**

* Implement realizer with a single prompt template; outputs `{question, options[], tip}`.

**Day 5**

* Build state store (answers/history). Add resume logic.

**Day 6**

* Author 30 gold scenarios; write order/coverage tests.

**Day 7**

* Add anti-regression CI, log every turn (state hash, prompt version, retrieval IDs).

---

## Example end-to-end call (deck)

**State (partial):**

```json
{
  "project_type": "deck",
  "answers": {"deck.project_kind":"new","deck.location":"attached"},
  "required_fields":[...],
  "history":[...]
}
```

**Planner →** `next_key = "deck.size"`

**Retriever →** (none needed; no facts required)

**Realizer →**

> “How big should the deck be? You can pick a size or say small/medium/large.”
> Options: 12×12 • 16×16 • 20×20 • Small • Not sure (upload photo)

**State saved**, repeat.

---

## Pro tips for scope-creation domains

* **Offer choices before free text.** It reduces ambiguity and speeds completion.
* **Always add a one-sentence “why this matters.”** Users comply more when they see the reason.
* **Branch hints early.** E.g., if composite → 12" OC joists; if elevation>30" → railings required.
* **Summarize every 4–5 questions** and ask “Keep going or adjust something?” (prevents user fatigue and captures corrections mid-flow).

---

If you want, I can package a **starter repo layout** (planner YAML, DSL, three prompts, gold scenarios, and a tiny FastAPI service) so you can drop it into Replit/GitHub and stop the regression today.
